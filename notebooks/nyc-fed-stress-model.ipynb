{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4ac26-bc0f-48e3-a519-99b2596d8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from datetime import date\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23e2cb-19a1-447a-a99e-0d52cb44f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://markets.newyorkfed.org\"\n",
    "\n",
    "START_DATE = \"2015-01-01\"    # adjust\n",
    "END_DATE   = str(date.today())\n",
    "\n",
    "# Outlier threshold in \"sigma units\" for tail probability\n",
    "TAIL_K = 3.0\n",
    "\n",
    "# Feature engineering\n",
    "ROLL_VOL_WINDOW = 20\n",
    "\n",
    "# Backtest controls (PyMC sampling is expensive)\n",
    "TRAIN_WINDOW_DAYS = 365 * 5\n",
    "STEP_DAYS = 10               # evaluate every ~2 weeks for speed\n",
    "DRAWS = 300\n",
    "TUNE  = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661c2bd-fefa-4f9c-9e17-b12861c5bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json(url: str, params: dict | None = None) -> dict:\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def find_records_list(obj: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    NY Fed responses usually contain a list of records somewhere.\n",
    "    This tries common keys and falls back to first list-of-dicts found.\n",
    "    \"\"\"\n",
    "    common_keys = [\"refRates\", \"rates\", \"data\", \"results\", \"observations\"]\n",
    "    for k in common_keys:\n",
    "        v = obj.get(k)\n",
    "        if isinstance(v, list) and (len(v) == 0 or isinstance(v[0], dict)):\n",
    "            return v\n",
    "    for v in obj.values():\n",
    "        if isinstance(v, list) and (len(v) == 0 or isinstance(v[0], dict)):\n",
    "            return v\n",
    "    raise ValueError(f\"Could not find records list in response keys={list(obj.keys())}\")\n",
    "\n",
    "def records_to_series(records: list[dict], name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Infer date+value fields for simple rate series.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return pd.Series(dtype=float, name=name)\n",
    "\n",
    "    date_keys = [\"effectiveDate\", \"date\", \"asOfDate\", \"timestamp\"]\n",
    "    val_keys  = [\"percentRate\", \"rate\", \"value\", \"pctRate\"]\n",
    "\n",
    "    sample = records[0]\n",
    "    dk = next((k for k in date_keys if k in sample), None)\n",
    "    vk = next((k for k in val_keys  if k in sample), None)\n",
    "\n",
    "    if dk is None or vk is None:\n",
    "        raise ValueError(f\"Unknown schema for {name}. Sample keys={list(sample.keys())}\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    idx = pd.to_datetime(df[dk]).dt.tz_localize(None)\n",
    "    s = pd.to_numeric(df[vk], errors=\"coerce\")\n",
    "    out = pd.Series(s.values, index=idx, name=name).sort_index()\n",
    "    out = out[~out.index.duplicated(keep=\"last\")]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5edd00-f254-4779-b2f9-ba8dc44fa0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFR endpoint pattern\n",
    "sofr_url = f\"{BASE}/api/rates/secured/sofr/search.json\"\n",
    "sofr_json = fetch_json(sofr_url, params={\"startDate\": START_DATE, \"endDate\": END_DATE})\n",
    "SOFR = records_to_series(find_records_list(sofr_json), \"SOFR\")\n",
    "\n",
    "# EFFR endpoint pattern\n",
    "effr_url = f\"{BASE}/api/rates/unsecured/effr/search.json\"\n",
    "effr_json = fetch_json(effr_url, params={\"startDate\": START_DATE, \"endDate\": END_DATE})\n",
    "EFFR = records_to_series(find_records_list(effr_json), \"EFFR\")\n",
    "\n",
    "print(\"SOFR head:\\n\", SOFR.dropna().head(3))\n",
    "print(\"\\nEFFR head:\\n\", EFFR.dropna().head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60aa0c6-6b4b-43a9-a354-d1527dd901e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rp_results(start_date: str, end_date: str, operation_types: str = \"repo,reverserepo\") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Pull repo + reverse repo operation results in one call.\n",
    "    operationTypes examples: \"repo\", \"reverserepo\", or \"repo,reverserepo\"\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/api/rp/results/search.json\"\n",
    "    params = {\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "        \"operationTypes\": operation_types,\n",
    "    }\n",
    "    js = fetch_json(url, params=params)\n",
    "\n",
    "    # Many RP responses look like: { \"repo\": { \"operations\": [...] } }\n",
    "    # We'll try common locations:\n",
    "    if \"repo\" in js and isinstance(js[\"repo\"], dict):\n",
    "        for k in [\"operations\", \"results\", \"data\"]:\n",
    "            if k in js[\"repo\"] and isinstance(js[\"repo\"][k], list):\n",
    "                return js[\"repo\"][k]\n",
    "\n",
    "    # fallback: search within response\n",
    "    try:\n",
    "        return find_records_list(js)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Could not locate operations list in rp response. keys={list(js.keys())}\")\n",
    "\n",
    "def rp_records_to_daily_volume(records: list[dict], name: str = \"RP_VOLUME\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Aggregate daily accepted amount across operations.\n",
    "    Attempts to infer:\n",
    "      - date field: operationDate / date / effectiveDate\n",
    "      - amount field: totalAmtAccepted / amountAccepted / totalAmount / amount\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return pd.Series(dtype=float, name=name)\n",
    "\n",
    "    sample = records[0]\n",
    "    date_keys = [\"operationDate\", \"date\", \"effectiveDate\", \"asOfDate\"]\n",
    "    amt_keys  = [\"totalAmtAccepted\", \"amountAccepted\", \"totalAmount\", \"amount\", \"amt\"]\n",
    "\n",
    "    dk = next((k for k in date_keys if k in sample), None)\n",
    "    ak = next((k for k in amt_keys  if k in sample), None)\n",
    "\n",
    "    if dk is None or ak is None:\n",
    "        print(\"Sample RP record:\\n\", json.dumps(sample, indent=2)[:2000])\n",
    "        raise ValueError(f\"Unknown RP schema. Need date+amount keys. sample_keys={list(sample.keys())}\")\n",
    "\n",
    "    df = pd.DataFrame(records).copy()\n",
    "    df[dk] = pd.to_datetime(df[dk]).dt.tz_localize(None)\n",
    "    df[ak] = pd.to_numeric(df[ak], errors=\"coerce\")\n",
    "\n",
    "    # daily sum of accepted amounts (repo + reverse repo together)\n",
    "    daily = df.groupby(df[dk].dt.normalize())[ak].sum().sort_index()\n",
    "    daily.name = name\n",
    "    return daily\n",
    "\n",
    "rp_records = fetch_rp_results(START_DATE, END_DATE, operation_types=\"repo,reverserepo\")\n",
    "RP_VOL = rp_records_to_daily_volume(rp_records, name=\"RP_VOLUME_TOTAL\")\n",
    "\n",
    "print(\"RP_VOL head:\\n\", RP_VOL.dropna().head(3))\n",
    "print(\"RP_VOL tail:\\n\", RP_VOL.dropna().tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cbe5f-122f-4d60-bd11-51ee60e0149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([SOFR, EFFR, RP_VOL], axis=1).dropna()\n",
    "\n",
    "# Price feature\n",
    "df[\"SPREAD_SOFR_EFFR\"] = df[\"SOFR\"] - df[\"EFFR\"]\n",
    "df[\"SPREAD_VOL_20D\"] = df[\"SPREAD_SOFR_EFFR\"].rolling(ROLL_VOL_WINDOW).std()\n",
    "\n",
    "# Quantity feature (stabilize with log; volumes can be huge / regime-shifting)\n",
    "df[\"RP_LOG\"] = np.log1p(df[\"RP_VOLUME_TOTAL\"])\n",
    "df[\"RP_LOG_CHG_5D\"] = df[\"RP_LOG\"].diff(5)\n",
    "df[\"RP_LOG_VOL_20D\"] = df[\"RP_LOG\"].rolling(ROLL_VOL_WINDOW).std()\n",
    "\n",
    "df = df.dropna()\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0018a99-7570-4578-96ca-7775a3341612",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df[[\"SOFR\", \"EFFR\"]].plot(title=\"SOFR vs EFFR\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df[\"SPREAD_SOFR_EFFR\"].plot(title=\"Spread: SOFR - EFFR\")\n",
    "plt.axhline(0.0)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df[\"RP_LOG\"].plot(title=\"Repo/Reverse Repo Volume (log1p aggregated)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b44c80-f253-4b96-b6ae-d5f32ac3c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_student_t_outlier_prob(\n",
    "    y_train: np.ndarray,\n",
    "    y_today: float,\n",
    "    tail_k: float = 3.0,\n",
    "    draws: int = 300,\n",
    "    tune: int = 300,\n",
    "):\n",
    "    # Robust prior scale\n",
    "    med = float(np.median(y_train))\n",
    "    mad = float(np.median(np.abs(y_train - med))) + 1e-6\n",
    "    scale = 1.4826 * mad\n",
    "\n",
    "    with pm.Model() as m:\n",
    "        mu = pm.Normal(\"mu\", mu=med, sigma=5 * scale)\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=5 * scale)\n",
    "        nu = pm.Exponential(\"nu\", lam=1/10) + 2  # heavy tails\n",
    "\n",
    "        pm.StudentT(\"obs\", nu=nu, mu=mu, sigma=sigma, observed=y_train)\n",
    "        idata = pm.sample(draws=draws, tune=tune, chains=4, target_accept=0.9, progressbar=True)\n",
    "\n",
    "    mu_s = idata.posterior[\"mu\"].values.reshape(-1)\n",
    "    sig_s = idata.posterior[\"sigma\"].values.reshape(-1)\n",
    "\n",
    "    z = (y_today - mu_s) / sig_s\n",
    "    p_tail = float(np.mean(np.abs(z) > tail_k))\n",
    "    return p_tail, idata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4283f5-e32a-4a02-aafa-7178b6504977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_p_tail(\n",
    "    y: pd.Series,\n",
    "    tail_k: float,\n",
    "    train_window_days: int = 365*5,\n",
    "    step: int = 10,\n",
    "    draws: int = 300,\n",
    "    tune: int = 300,\n",
    ") -> pd.Series:\n",
    "    y = y.dropna()\n",
    "    out = []\n",
    "\n",
    "    for i in range(train_window_days, len(y), step):\n",
    "        window = y.iloc[i-train_window_days : i+1]\n",
    "        y_train = window.iloc[:-1].values.astype(float)\n",
    "        y_today = float(window.iloc[-1])\n",
    "\n",
    "        try:\n",
    "            p_tail, _ = fit_student_t_outlier_prob(y_train, y_today, tail_k=tail_k, draws=draws, tune=tune)\n",
    "        except Exception:\n",
    "            p_tail = np.nan\n",
    "\n",
    "        out.append((y.index[i], p_tail))\n",
    "\n",
    "    return pd.Series([v for _, v in out], index=[d for d, _ in out], name=f\"p_tail_{y.name}\")\n",
    "\n",
    "# Two separate models\n",
    "p_spread = walk_forward_p_tail(df[\"SPREAD_SOFR_EFFR\"], TAIL_K, TRAIN_WINDOW_DAYS, STEP_DAYS, DRAWS, TUNE)\n",
    "p_repo   = walk_forward_p_tail(df[\"RP_LOG_CHG_5D\"], TAIL_K, TRAIN_WINDOW_DAYS, STEP_DAYS, DRAWS, TUNE)\n",
    "\n",
    "p_spread.dropna().tail(), p_repo.dropna().tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3d485-292e-4bd5-b694-8162d5054c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.concat([p_spread, p_repo], axis=1).dropna()\n",
    "p.columns = [\"p_spread\", \"p_repo\"]\n",
    "\n",
    "p[\"p_stress_indep\"] = 1.0 - (1.0 - p[\"p_spread\"]) * (1.0 - p[\"p_repo\"])\n",
    "p[\"p_stress_max\"]   = np.maximum(p[\"p_spread\"], p[\"p_repo\"])\n",
    "\n",
    "p.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f368d95-db7a-49c6-9219-da28e906ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "p[[\"p_spread\", \"p_repo\"]].plot(title=\"Outlier probabilities (walk-forward)\")\n",
    "plt.axhline(0.10, linestyle=\"--\")\n",
    "plt.axhline(0.25, linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "p[[\"p_stress_indep\", \"p_stress_max\"]].plot(title=\"Combined stress probability\")\n",
    "plt.axhline(0.25, linestyle=\"--\")\n",
    "plt.axhline(0.50, linestyle=\"--\")\n",
    "plt.axhline(0.80, linestyle=\"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d650ee-000c-41e2-9611-628835a301de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_windows = [\n",
    "    (\"2019-09-10\", \"2019-10-15\", \"Repo spike\"),\n",
    "    (\"2020-02-20\", \"2020-05-15\", \"COVID\"),\n",
    "    (\"2023-03-08\", \"2023-05-01\", \"Regional banks\"),\n",
    "]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = p[\"p_stress_indep\"].plot(title=\"Combined stress probability with stress windows\")\n",
    "\n",
    "for s, e, label in stress_windows:\n",
    "    ax.axvspan(pd.to_datetime(s), pd.to_datetime(e), alpha=0.2)\n",
    "ax.axhline(0.80, linestyle=\"--\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
